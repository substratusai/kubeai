# Source: models/templates/models.yaml
apiVersion: kubeai.org/v1
kind: Model
metadata:

  name: llama-3.1-8b-instruct-cpu
spec:
  features: [TextGeneration]
  owner: meta-llama
  url: hf://meta-llama/Meta-Llama-3.1-8B-Instruct
  engine: VLLM
  args:
  
    - --max-model-len=32768
    - --max-num-batched-token=32768
  env:
  
    VLLM_CPU_KVCACHE_SPACE: "4"
  resourceProfile: cpu:6
