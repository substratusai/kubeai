# Default values for kubeai.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

secrets:
  huggingface:
    create: true
    token: ""
    # The name of the secret to use.
    # If not set, and create is true, the name is generated using the fullname template.
    # The token value is pulled from the key, "token".
    name: ""

modelServers:
  VLLM:
    images:
      # The key is the image name (referenced from resourceProfiles) and the value is the image.
      # The "default" image should always be specified.
      # "default" is used when no imageName is specified or if a specific image is not found.
      default: "vllm/vllm-openai:v0.5.5"
      cpu: "substratusai/vllm:v0.5.5-cpu"
      nvidia-gpu: "vllm/vllm-openai:v0.5.5"
      # TODO publish tpu image.
      google-tpu: "substratusai/vllm-openai-tpu:v0.5.5"
  OLlama:
    images:
      default: "ollama/ollama:latest"
  FasterWhisper:
    images:
      default: "fedirz/faster-whisper-server:latest-cpu"
      nvidia-gpu: "fedirz/faster-whisper-server:latest-cuda"

resourceProfiles:
  cpu:
    imageName: "cpu"
    requests:
      cpu: 1
      memory: "2Gi"
  nvidia-gpu-l4:
    imageName: "nvidia-gpu"
    limits:
      nvidia.com/gpu: "1"
    requests:
      nvidia.com/gpu: "1"
      cpu: "6"
      memory: "24Gi"
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"
  # nvidia-gpu-v100:
  # nvidia-gpu-a100:
  # google-tpu-v5e:
  #   imageName: "google-tpu"
  #   requests:
  #     google.com/tpu: 4
  #   limits:
  #     google.com/tpu: 4
  #   nodeSelector:
  #     cloud.google.com/gke-accelerator-type: tpu-v5-lite-podslice

modelAutoscaling:
  # Interval that the autoscaler will scrape model server metrics.
  # and calculate the desired number of replicas.
  interval: 10s
  # Time window the autoscaling algorithm will consider when calculating
  # the desired number of replicas.
  timeWindow: 10m

messaging:
  errorMaxBackoff: 30s
  streams: []

# Configure the openwebui subchart.
openwebui:
  fullnameOverride: "openwebui"
  image:
    tag: v0.3.19
  env:
  - name: WEBUI_AUTH
    value: "False"
  - name: OPENAI_API_KEYS
    value: "not-used"
  # - name: OPENAI_API_BASE_URL
  #  # TODO: This changes with .fullnameOverride and .service.port, make this more robust.
  #  value: "http://kubeai/openai/v1"
  - name: OPENAI_API_BASE_URLS
    # TODO: This changes with .fullnameOverride and .service.port, make this more robust.
    value: "http://kubeai/openai/v1"
  # A good number of features are not compatible with the KubeAI architecture.
  - name: ENABLE_OLLAMA_API
    value: "false"
  - name: SHOW_ADMIN_DETAILS
    value: "false"
  - name: SAFE_MODE
    value: "true"
  - name: ENABLE_LITELLM
    value: "false"

replicaCount: 1

image:
  repository: substratusai/kubeai
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: "kubeai"

command:
- /app/manager

args: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext:
  runAsNonRoot: true
  # fsGroup: 2000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  # readOnlyRootFilesystem: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  rules:
    - host: kubeai.example.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

livenessProbe:
  httpGet:
    path: /healthz
    port: 8081
  initialDelaySeconds: 120
  periodSeconds: 20

readinessProbe:
  httpGet:
    path: /readyz
    port: 8081
  initialDelaySeconds: 5
  periodSeconds: 10

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}
